{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05f9d8-5316-4186-a816-b5092fcf363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd         # 导入pandas库，它就像一个超级强大的Excel工具，专门用来处理表格数据（DataFrame）。\n",
    "\n",
    "#读取一个Excel文件，并将读取到的数据存储在变量data中\n",
    "data = pd.read_excel('大学生低碳生活行为的影响因素数据集.xlsx')\n",
    "# 使用pandas的read_excel函数，读取名为 '大学生低碳生活行为的影响因素数据集.xlsx' 的Excel文件。\n",
    "# 将文件内容加载到一个名为 'data' 的DataFrame（可以理解为表格）中。\n",
    "\n",
    "#打印出数据集的前5行\n",
    "print(data.head())          # 使用DataFrame的 .head() 方法，显示数据的前5行。\n",
    "                            # 这能帮助我们快速查看数据的基本结构、列名和前几条记录，初步了解数据长什么样。\n",
    "\n",
    "#处理数据集中的缺失值\n",
    "initial_row_count = data.shape[0] #或者Len(data)  #处理前的数据行数\n",
    "# data.shape[0]：获取DataFrame 'data' 的行数。这是在处理缺失值之前的数据量。\n",
    "# 将这个初始行数存储在 `initial_row_count` 变量中，以便后续计算删除了多少行。\n",
    "data = data.dropna()                #删除缺失值所在行\n",
    "# 使用DataFrame的 .dropna() 方法，删除所有包含任何缺失值（NaN）的行。\n",
    "# 这样做是为了确保我们处理的数据是完整的，避免缺失值对后续分析造成影响。\n",
    "# 将处理后的结果重新赋值给 'data'，覆盖掉原始数据。\n",
    "final_row_count = data.shape[0]     #处理后的数据行数\n",
    "# 再次获取DataFrame 'data' 的行数，这时是处理完缺失值之后的数据量。\n",
    "# 将这个最终行数存储在 `final_row_count` 变量中。\n",
    "print(f'处理后数据行数: {final_row_count}, 删除的行数: {initial_row_count - final_row_count}')\n",
    "# 打印一条消息，显示处理后的数据总行数，以及通过比较前后行数，计算并打印出删除了多少行。\n",
    "\n",
    "#删除重复行\n",
    "data = data.drop_duplicates()\n",
    "# 使用DataFrame的 .drop_duplicates() 方法，删除DataFrame中所有完全相同的重复行。\n",
    "# 例如，如果两行数据的所有列值都一模一样，那么其中一行就会被删除。\n",
    "# 这样可以确保数据集中的每一行都是独一无二的，避免重复数据对分析结果产生偏差。\n",
    "# 将处理后的结果重新赋值给 'data'，覆盖掉原始数据。\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # 从scikit-learn库中导入StandardScaler，这是一个用于数据标准化的工具。\n",
    "\n",
    "# 对数值型数据进行标准化处理\n",
    "numerical_features = ['4.您的月生活费○≦1,000元   ○1,001-2,000元   ○2,001-3,001元   ○≧3,001元'] # 这里列名看起来是一个调查问卷的问题，\n",
    "                                                                                                    # 且可能直接包含选项文本，这种特征通常是分类或序数型，\n",
    "                                                                                                    # 对它直接进行StandardScaler标准化可能不是最好的处理方式，\n",
    "                                                                                                    # 除非实际数据值是数值编码后的结果。\n",
    "# 定义一个列表，包含我们想要进行标准化处理的数值型特征（列名）。\n",
    "scaler = StandardScaler()           # 创建一个StandardScaler的实例（一个“标准化工具”）。\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "# data[numerical_features]：选取DataFrame中所有需要标准化的列。\n",
    "# scaler.fit_transform(...)：这是StandardScaler的核心方法。它做了两件事：\n",
    "#   1. `fit`：计算选定列的平均值（mean）和标准差（std）。\n",
    "#   2. `transform`：对选定列中的每个值进行标准化处理，具体公式是：(原始值 - 平均值) / 标准差。\n",
    "# 经过标准化后，这些列的数据将服从标准正态分布（平均值为0，标准差为1），这有助于许多机器学习模型更好地工作。\n",
    "# 将标准化后的结果重新赋值回 'data' 的相应列中。\n",
    "\n",
    "#选择特征\n",
    "selected_features = [item for index,item in enumerate(data) if 2<=index<=9] #或者一个一个打进去\n",
    "# 这行代码的目的是选择DataFrame中索引为2到9（包含2和9）的列作为特征。\n",
    "# enumerate(data)：`data` 是一个DataFrame，直接迭代它会得到列名。`enumerate` 会为每个列名生成一个索引和列名的组合。\n",
    "# [item for index,item in enumerate(data) if 2<=index<=9]：这是一个列表推导式。\n",
    "# 它会遍历所有的 (索引, 列名) 对，并只选择那些索引在2到9之间的列名，将它们收集成一个列表。\n",
    "# 这样，`selected_features` 列表就包含了从第3列到第10列（因为索引从0开始）的所有列名。\n",
    "X = data[selected_features]         # 将 'data' 中由 `selected_features` 列表指定的列抽取出来，赋值给变量 'X'。\n",
    "                                    # 'X' 通常代表机器学习中的特征矩阵（或输入数据），是模型用来学习的输入。\n",
    "print(X)                            # 打印特征矩阵 'X'，显示选定特征的数据内容。\n",
    "\n",
    "# 创建目标变量\n",
    "y = data['低碳行为积极性']          # 将 'data' 中的 '低碳行为积极性' 列抽取出来，赋值给变量 'y'。\n",
    "                                    # 'y' 通常代表机器学习中的目标变量（或因变量，我们希望预测的值）。\n",
    "\n",
    "from sklearn.model_selection import train_test_split # 从scikit-learn库中导入train_test_split，这是一个用于分割数据集的工具。\n",
    "\n",
    "# 数据划分（测试集取20%）\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# train_test_split 是一个非常重要的函数，它将我们的数据集分割成四个部分：\n",
    "#   X_train：用于训练模型的特征数据（80%）。\n",
    "#   X_test：用于评估模型的特征数据（20%，模型在训练中看不到）。\n",
    "#   y_train：用于训练模型的目标变量（80%）。\n",
    "#   y_test：用于评估模型的目标变量（20%）。\n",
    "# test_size=0.2：指定测试集占总数据集的比例是20%（即0.2），那么训练集就占80%。\n",
    "# random_state=42：这是一个随机种子。设置这个参数可以确保每次运行代码时，数据集的划分结果都是相同的，\n",
    "# 这样保证了实验的可复现性。\n",
    "\n",
    "# 合并处理后得数据，并将其保存（保存中不用额外创建索引）\n",
    "cleaned_data = pd.concat([X,y], axis=1)\n",
    "# pd.concat([...], axis=1)：这是pandas的一个函数，用于沿特定轴连接（合并）多个DataFrame或Series。\n",
    "# [X,y]：表示要合并 'X'（特征矩阵）和 'y'（目标变量）。\n",
    "# axis=1：表示沿着列方向（水平方向）进行合并。\n",
    "# 结果是 `cleaned_data` DataFrame 会包含所有选定的特征列和 '低碳行为积极性' 目标列，形成一个完整的、清洗和处理后的数据集。\n",
    "cleaned_data.to_csv('2.1.2_cleaned_data.csv', index=False)\n",
    "# to_csv(...)：这是pandas用来将DataFrame保存为CSV文件的方法。\n",
    "# '2.1.2_cleaned_data.csv'：指定新保存的文件的名字。\n",
    "# index=False：这个参数非常重要，它表示在保存CSV文件时，不要把DataFrame的索引（左边默认从0开始的数字序号）也写入到CSV文件中。\n",
    "# 这样做可以避免文件中有额外且无用的列。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
