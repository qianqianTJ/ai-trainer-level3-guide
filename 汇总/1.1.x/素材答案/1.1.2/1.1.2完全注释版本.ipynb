{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1dfa0-bcc5-443b-9776-92356ff20cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd         # 导入pandas库，它就像一个超级强大的Excel工具，用来处理表格数据非常方便。\n",
    "import numpy as np          # 导入numpy库，它主要用于进行数值计算，特别是在处理数组（例如一堆数字）时效率很高。\n",
    "import matplotlib.pyplot as plt # 导入matplotlib.pyplot库，这个库是用来画图的，比如折线图、柱状图等，让数据可视化。\n",
    "\n",
    "# 读取数据集 2分\n",
    "data = pd.read_csv('sensor_data.csv') # 使用pandas的read_csv函数，读取名为 'sensor_data.csv' 的文件。\n",
    "                                       # 这就相当于我们打开这个CSV文件，把里面的内容读取到一个叫做 'data' 的表格（DataFrame）里。\n",
    "print(data)                            # 打印出 'data' 这个表格的内容，让我们能看到原始数据的样子。\n",
    "\n",
    "# 1. 传感器数据统计\n",
    "# 对传感器类型进行分组，并计算每个组的数据数量和平均值 3分\n",
    "sensor_stats = data.groupby(data['SensorType'])['Value'].agg(['count','mean'])\n",
    "# 让我们一步步拆解这行代码：\n",
    "# data.groupby(data['SensorType'])：这行代码的意思是，把我们的大表格 'data' 按照 'SensorType'（传感器类型）这一列的值进行分组。\n",
    "# 比如，如果数据里有 'Temperature'（温度）和 'Humidity'（湿度）两种传感器，它就会把所有温度数据分到一组，所有湿度数据分到另一组。\n",
    "# ['Value']：在每个分组内部，我们只关心 'Value'（传感器数值）这一列的数据。\n",
    "# .agg(['count','mean'])：'agg' 是 'aggregate' 的缩写，意思是“聚合”。\n",
    "# 它会把之前分组后的 'Value' 列数据做两个计算：\n",
    "#   'count'：统计每个分组里有多少个数据点，也就是某个类型传感器有多少条记录。\n",
    "#   'mean'：计算每个分组里 'Value' 的平均值，也就是某个类型传感器的平均数值。\n",
    "# 最终，'sensor_stats' 这个变量就会存储一个统计表，显示每种传感器类型的数据数量和平均值。\n",
    "\n",
    "# 输出结果\n",
    "print(\"传感器数据数量和平均值:\") # 打印一个标题，说明接下来要显示什么。\n",
    "print(sensor_stats)                # 打印出 'sensor_stats' 这个统计表的内容。\n",
    "\n",
    "# 2. 按位置统计温度和湿度数据\n",
    "# 筛选出温度和湿度数据，然后按位置和传感器类型分组，计算每个组的平均值 2分\n",
    "location_stats = data[data['SensorType'].isin(['Temperature','Humidity'])].groupby(['Location','SensorType'])['Value'].mean().unstack()\n",
    "# 这行代码稍微复杂一些，我们慢慢来分解：\n",
    "# data['SensorType'].isin(['Temperature','Humidity'])：这是一个筛选条件。\n",
    "# 它检查 'SensorType' 这一列的值，看它是不是在列表 `['Temperature','Humidity']` 里面。\n",
    "# 如果是，结果就是 True；如果不是，结果就是 False。\n",
    "# data[...]：用上面生成的 True/False 系列来筛选原始的 'data' 表格。\n",
    "# 只有那些 'SensorType' 是 'Temperature' 或 'Humidity' 的行才会被保留下来。\n",
    "# .groupby(['Location','SensorType'])：对筛选后的数据进行分组，这次是同时按照 'Location'（位置）和 'SensorType'（传感器类型）两列进行分组。\n",
    "# 比如，会有一个“客厅-温度”组，一个“客厅-湿度”组，一个“卧室-温度”组等等。\n",
    "# ['Value'].mean()：在每个分组内部，计算 'Value' 列的平均值。\n",
    "# .unstack()：这是一个非常酷的pandas操作，它的作用是“旋转”表格。\n",
    "# 想象一下，分组后的结果可能是：\n",
    "# Location  SensorType\n",
    "# 客厅      Humidity   25.0\n",
    "#         Temperature 22.5\n",
    "# 卧室      Humidity   30.0\n",
    "#         Temperature 20.0\n",
    "# unstack() 会把 'SensorType' 这一层级从行索引变成列标题，让表格看起来更像我们常用的交叉表：\n",
    "# SensorType  Humidity  Temperature\n",
    "# Location\n",
    "# 客厅           25.0        22.5\n",
    "# 卧室           30.0        20.0\n",
    "# 这样，我们就能更直观地看到每个位置的平均温度和平均湿度。\n",
    "# 最终，'location_stats' 存储的就是这个经过旋转的平均值表格。\n",
    "\n",
    "# 输出结果\n",
    "print(\"每个位置的温度和湿度数据平均值:\") # 打印一个标题。\n",
    "print(location_stats)                  # 打印出 'location_stats' 这个统计表的内容。\n",
    "\n",
    "# 3. 数据清洗和异常值处理\n",
    "# 标记异常值 3分\n",
    "data['is_abnormal'] = np.where(\n",
    "    # 这是一个条件非常复杂的 np.where，用来判断每个数据是不是异常值。\n",
    "    # 我们一层层地看：\n",
    "    # 外层 np.where(条件, 如果条件为真赋的值, 如果条件为假赋的值)\n",
    "    # 如果条件成立，data['is_abnormal'] 列会赋值为 True，否则赋值为 False。\n",
    "\n",
    "    # 来看“条件”的部分：\n",
    "    ((data['SensorType']=='Temperature') & ((data['Value'] < -10) | (data['Value'] > 50))) |\n",
    "    # 这一部分是针对温度传感器数据的异常值判断：\n",
    "    # (data['SensorType']=='Temperature')：首先判断传感器类型是不是“Temperature”（温度）。\n",
    "    # & ( ... )：如果确实是温度传感器，接着判断后面的条件（逻辑“与”）。\n",
    "    # ((data['Value'] < -10) | (data['Value'] > 50))：判断温度值是不是小于-10度 或 大于50度（逻辑“或”）。\n",
    "    # 也就是说，如果传感器是温度类型，并且数值低于-10或高于50，那么它就是异常。\n",
    "\n",
    "    ((data['SensorType']=='Humidity') & ((data['Value'] < 0) | (data['Value'] > 100))),\n",
    "    # 这一部分是针对湿度传感器数据的异常值判断：\n",
    "    # (data['SensorType']=='Humidity')：判断传感器类型是不是“Humidity”（湿度）。\n",
    "    # & ( ... )：如果确实是湿度传感器，接着判断后面的条件。\n",
    "    # ((data['Value'] < 0) | (data['Value'] > 100))：判断湿度值是不是小于0% 或 大于100%（因为湿度百分比通常在0到100之间）。\n",
    "    # 也就是说，如果传感器是湿度类型，并且数值低于0或高于100，那么它就是异常。\n",
    "\n",
    "    # 最后的逻辑是：\n",
    "    # 第一个椭圆(...) | 第二个椭圆(...)：两个大条件的逻辑“或”。\n",
    "    # 意味着，只要满足“温度异常”或“湿度异常”中的任何一个，整个条件就为 True。\n",
    "    True, # 如果上面的整个大条件为 True，那么'is_abnormal'列对应的值就是 True。\n",
    "    False # 如果上面的整个大条件为 False，那么'is_abnormal'列对应的值就是 False。\n",
    ")\n",
    "# 总结：这行代码就是根据传感器的类型和数值范围，智能化地给每条数据贴上“是不是异常值”的标签。\n",
    "\n",
    "# 输出异常值数量 2分\n",
    "print(\"异常值数量:\", data['is_abnormal'].sum())\n",
    "# data['is_abnormal']：包含 True/False 标签的列（True表示异常，False表示正常）。\n",
    "# .sum()：对这个布尔值（True/False）系列进行求和。在Python中，True被看作1，False被看作0。\n",
    "# 所以，求和的结果就是所有 True 的数量，也就是异常值的总数量。\n",
    "# 这种方法比 .value_counts() 更直接地获取异常值的总数。\n",
    "\n",
    "# 填补缺失值\n",
    "# 使用前向填充和后向填充的方法填补缺失值 4分\n",
    "data['Value'].fillna(method='ffill', inplace=True)\n",
    "# data['Value']：我们关注的是 'Value'（传感器数值）这一列，因为数据通常会在这一列出现缺失。\n",
    "# .fillna(method='ffill', inplace=True)：这是用来填充缺失值（NaN）的方法。\n",
    "# method='ffill'：'ffill' 是 'forward fill' 的缩写，意思是“前向填充”。\n",
    "# 它会用前一个有效的数据值来填充当前遇到的缺失值。\n",
    "# 举例：如果数据是 [10, NaN, 12]，经过 'ffill' 填充后会变成 [10, 10, 12]。\n",
    "# inplace=True：这个参数很重要，它表示直接修改原始的 'data' DataFrame，而不是创建一个新的修改后的副本。\n",
    "# 也就是说，这行代码会直接修改 'data' 表格的 'Value' 列。\n",
    "\n",
    "data['Value'].fillna(method='bfill', inplace=True)\n",
    "# .fillna(method='bfill', inplace=True)：这和上面一行类似，但是使用了 'bfill'。\n",
    "# method='bfill'：'bfill' 是 'backward fill' 的缩写，意思是“后向填充”。\n",
    "# 它会用后一个有效的数据值来填充当前遇到的缺失值。\n",
    "# 举例：如果数据是 [NaN, 10, 12]，经过 'bfill' 填充后会变成 [10, 10, 12]。\n",
    "# 为什么要用两次填充呢？因为 'ffill' 无法填充第一个值就是NaN的情况，'bfill' 无法填充最后一个值是NaN的情况。\n",
    "# 组合使用可以更全面地处理缺失值，确保所有缺失值都被尽可能合理地填补。\n",
    "\n",
    "# 保存清洗后的数据\n",
    "# 删除用于标记异常值的列，并将清洗后的数据保存到新的CSV文件中 4分\n",
    "cleaned_data = data.drop(columns=['is_abnormal'])\n",
    "# data.drop(columns=['is_abnormal'])：'drop' 方法用于删除DataFrame中的行或列。\n",
    "# columns=['is_abnormal']：指定我们要删除的是名为 'is_abnormal' 的列。\n",
    "# 因为 'is_abnormal' 列只是我们在数据清洗过程中用于标记异常值的临时列，现在数据处理完了，就不需要它了。\n",
    "# cleaned_data = ...：将删除列后的新DataFrame赋值给 'cleaned_data' 这个变量。\n",
    "\n",
    "cleaned_data.to_csv('cleaned_sensor_data.csv', index=False)\n",
    "# cleaned_data.to_csv(...)：这是pandas用来将DataFrame保存为CSV文件的方法。\n",
    "# 'cleaned_sensor_data.csv'：指定新保存的文件的名字。\n",
    "# index=False：这个参数很重要，它表示在保存CSV文件时，不要把DataFrame的索引（左边默认从0开始的数字序号）也写入到CSV文件中。\n",
    "# 如果不设置这个参数，CSV文件里会多一列无用的索引号。\n",
    "print(\"数据清洗完成，已保存为 'cleaned_sensor_data.csv'\") # 打印一条消息，告诉用户数据已经清洗完成并保存。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
